\documentclass[conference]{IEEEtran}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{xurl}
\usepackage{hyperref}

\title{Vote Tracing: Model-Level Explainability for RF Signal Classification Ensembles}
\author{\IEEEauthorblockN{Benjamin J. Gilbert}
\IEEEauthorblockA{Spectrcyde\\
Email: github.bgilbert1984@gmail.com}}

\begin{document}
\maketitle

\begin{abstract}
We convert per-model votes into auditable traces and exact Shapley attributions for RF ensemble decisions. We expose hooks in \texttt{classify\_signal()} to log per-model logits, calibrated probabilities, weights, and OSR gates, enabling timeline and contribution analyses with negligible overhead. Our approach provides interpretable explanations for ensemble classifications through vote tracing, exact model attribution (8--220 $\mu$s for typical RF ensembles), and disagreement analysis, enhancing trust and debugging capabilities for RF signal classification systems.
\end{abstract}

\begin{IEEEkeywords}
Explainable AI, ensemble methods, RF signal classification, exact Shapley values, vote attribution, open-set rejection
\end{IEEEkeywords}

\section{Introduction}

Ensemble methods have proven highly effective for RF signal classification, combining multiple models to achieve superior accuracy and robustness~\cite{dietterich2000ensemble}. However, the decision-making process within ensembles remains opaque, making it difficult to understand why certain classifications are made or to debug model failures. This lack of interpretability is particularly problematic in critical applications where understanding the reasoning behind predictions is essential.

We address this challenge by introducing a comprehensive vote tracing system that captures detailed information about ensemble decision-making processes. Our approach records per-model predictions, confidence scores, and intermediate computations, then applies Shapley-like attribution methods to quantify each model's contribution to the final decision.

\section{Audit Hook Architecture}

\subsection{Vote Trace Recording}

We instrument the \texttt{classify\_signal()} method with lightweight audit hooks that record comprehensive metadata about the ensemble decision process. Our system captures:

\begin{itemize}
\item \textbf{Per-model logits and probabilities:} Raw and temperature-scaled outputs from each ensemble member
\item \textbf{Model weights and temperatures:} Configuration parameters affecting vote aggregation
\item \textbf{Timing information:} Latency measurements for performance analysis
\item \textbf{Aggregate statistics:} Final probabilities, entropy, and confidence margins
\item \textbf{Open-set detection:} OSR gate decisions and associated metrics when available
\end{itemize}

The audit data is stored in \texttt{signal.metadata["ensemble\_trace"]} as a structured log of events, enabling retrospective analysis without affecting runtime performance.

\subsection{Shapley Attribution (Exact)}

We attribute each model's contribution using \emph{exact} Shapley values from cooperative game theory~\cite{shapley1953value}. The players are the $M$ ensemble members ($M=5$--$10$ in all experiments) and the characteristic function $f(S)$ is the arithmetic mean of the target-class probabilities from the models in coalition $S$ ($f(\emptyset)=0$).

Because $M$ is deliberately small, we compute the Shapley values \emph{exactly} via subset enumeration in $O(M \cdot 2^M)$ scalar operations:

$$
\phi_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!\,(M-|S|-1)!}{M!} \bigl[f(S \cup \{i\}) - f(S)\bigr]
$$

No additional model forwards are required — we reuse the per-model target-class probabilities already logged in the vote trace.

Measured cost (pure Python, i7-13700K):
\begin{itemize}
    \item $M=5$ $\to$ 8 $\mu$s
    \item $M=8$ $\to$ 45 $\mu$s  
    \item $M=10$ $\to$ 220 $\mu$s
    \item $M=12$ $\to$ 0.8 ms
\end{itemize}

These timings are 2--3 orders of magnitude faster than the neural network inference itself and introduce zero Monte Carlo noise. For $M>20$ (rare in RF ensembles) the system falls back to high-quality permutation sampling.

All results in this paper (Table~I, Figs.~2--3) use \emph{exact} Shapley values.

\section{Visualization and Analysis}

\subsection{Vote Timeline Analysis}

Vote timelines visualize per-model probabilities for the predicted class alongside the final ensemble probability. These plots reveal:
\begin{itemize}
\item Model-level confidence variations
\item Outlier models that disagree with the ensemble
\item The effect of vote aggregation on final confidence
\end{itemize}

\subsection{Contribution Attribution}

Shapley contribution plots show each model's positive or negative influence on the final prediction. This enables:
\begin{itemize}
\item Identification of key contributing models
\item Detection of models with negative impact
\item Quantification of model importance for specific signals
\end{itemize}

\subsection{Open-Set Rejection via Vote Traces}

Real-world RF deployments routinely encounter unknown modulations, jammers, or novel emitters not seen during training. Our vote tracing system enables powerful open-set rejection (OSR) with \textbf{zero additional inference overhead} — all required signals (per-model logits, probabilities, and disagreement statistics) are already captured in \texttt{signal.metadata["ensemble\_trace"]}.

We support multiple state-of-the-art OSR methods out of the box:

\begin{itemize}
\item Max-probability + entropy gating (default thresholds $\tau_p = 0.60$, $\tau_H = 1.2$)
\item Energy-based scoring~\cite{liu2020energy} on averaged ensemble logits (our primary baseline)
\item Simplified OpenMax-style Weibull tail modeling on per-class mean activation vectors
\end{itemize}

Crucially, the vote trace provides a \textbf{novel ensemble disagreement signal} — the standard deviation $\sigma_p(y^*)$ of per-model target-class probabilities. High-confidence known signals show strong model agreement (low $\sigma_p$); unknowns typically cause inconsistent or artificially over-confident individual predictions (high $\sigma_p$).

We combine energy score $E$ with disagreement via the tuned rule:  
\textbf{OSR score = $E$ – $\lambda \times \sigma_p(y^*)$} ($\lambda = 10.2$ in all experiments).  
Reject if score $< \tau$ ($\tau$ chosen for $\approx$95\% known-class coverage).

\textbf{Comparison to ODIN}~\cite{liang2018enhancing}  
ODIN (2018) remains a popular baseline that widens the in-/out-of-distribution softmax gap via temperature scaling and small gradient-based input perturbations ($\|\varepsilon\|_\infty \leq \varepsilon$, $\varepsilon$ typically 0.001–0.004). While effective on CIFAR/SVHN benchmarks, ODIN has three practical drawbacks in RF systems:

\begin{enumerate}
\item \textbf{Higher runtime overhead} — requires an extra forward+backward pass per signal for perturbations ($\approx$1.8–2.2$\times$ inference time in our tests).
\item \textbf{Gradient requirement} — fails on frozen/deployed models or when gradients are unavailable (common in edge RF hardware).
\item \textbf{Superseded performance} — energy-based scoring alone already outperforms ODIN by 4–12\% AUROC on dense prediction tasks~\cite{liu2020energy}, and adding our disagreement signal further improves separation without perturbations.
\end{enumerate}

\begin{table}[t]
\centering
\caption{Open-Set Performance at $\approx$95\% Known-Class Coverage (RML2018.01a known classes + 2000 simulated unknowns: LoRa, Zigbee, 5G NR FR1, pulsed radar, CDMA; SNR –10 to +12 dB)}
\label{tab:osr_comparison}
\begin{tabular}{lccccc}
\toprule
Method & Known Acc. & Unknown Rej. & AUROC & Extra Forwards & Gradients \\
\midrule
Max-Prob + Entropy & 95.3\% & 89.1\% & 0.964 & 0 & No \\
ODIN ($T=1000$, $\varepsilon=0.002$) & 95.7\% & 91.4\% & 0.975 & 1 & Yes \\
Energy-only & 96.1\% & 92.8\% & 0.980 & 0 & No \\
Energy + Disagreement (ours) & \textbf{96.5\%} & \textbf{95.3\%} & \textbf{0.988} & 0 & No \\
\bottomrule
\end{tabular}
\end{table}

Our method achieves \textbf{+3.9 pp unknown rejection over ODIN} (and +2.5 pp over energy-only) while being \textbf{2$\times$ faster and gradient-free} — ideal for real-time spectrum monitoring. The disagreement signal is unique to ensembles and comes essentially for free from vote traces.

All OSR decisions, per-model distances, Weibull parameters, energy scores, and $\sigma_p$ values are logged in \texttt{signal.metadata["osr"]}, enabling full auditability and dynamic threshold adaptation without re-inference.

\section{Figures}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/vote_timeline_0.pdf}
\caption{Vote timeline for an exemplar signal showing per-model probabilities for the predicted class (circles) and final ensemble probability (dashed line).}
\label{fig:vote_timeline}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/vote_shapley_mean.pdf}
\caption{Mean Shapley-like contribution over the dataset. Positive values indicate models that typically increase prediction confidence, while negative values indicate models that typically decrease confidence.}
\label{fig:shapley_mean}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/model_agreement_matrix.pdf}
\caption{Model agreement matrix showing pairwise agreement rates across all samples. Diagonal elements are always 1.0 (self-agreement).}
\label{fig:agreement}
\end{figure}

\section{Tables}

Our system generates quantitative summaries of model contributions and performance characteristics.

\input{tables/vote_contrib_table.tex}

\input{tables/shapley_ablation_table.tex}

\section{Implementation Details}

The vote tracing system adds minimal overhead to ensemble classification. Audit hooks execute in approximately 0.1-0.5ms per signal, while Shapley computation scales as $O(M \cdot S)$ where $M$ is the number of models and $S$ is the number of Monte Carlo samples.

Memory usage is proportional to the trace history length, typically requiring 1-2KB per classified signal for metadata storage. The system supports both batch and streaming analysis modes.

\section{Applications}

Vote traces enable several practical applications:

\begin{itemize}
\item \textbf{Model debugging:} Identify consistently underperforming ensemble members
\item \textbf{Dataset analysis:} Find signals where models systematically disagree
\item \textbf{Confidence calibration:} Analyze the relationship between ensemble confidence and prediction accuracy
\item \textbf{Adversarial detection:} Detect unusual voting patterns that may indicate adversarial inputs
\end{itemize}

\section{Related Work}

Ensemble interpretability has been explored in various domains~\cite{breiman2001random,zhou2012ensemble}. However, most existing approaches focus on feature importance rather than model-level contributions. 

Recent work by Rozemberczki and Sarkar~\cite{rozemberczki2021troupe} formalized ensemble Shapley values using cooperative game theory, establishing theoretical foundations for model-level attribution in voting scenarios. Kim~\cite{kim2024ensemble_shapley} demonstrated that individual model importance based on contribution to ensemble accuracy provides superior insights compared to traditional ensemble diversity metrics.

Our exact Shapley computation builds on these theoretical foundations while addressing computational efficiency for small RF ensembles. The Model Class Reliance framework of Fisher et al.~\cite{fisher2019mcr} shares our focus on understanding "variable importance by studying an entire class of prediction models simultaneously," but applies to feature-level rather than model-level analysis.

The vote tracing approach is inspired by cooperative game theory applications in machine learning~\cite{lundberg2017unified} but adapted specifically for RF ensemble voting scenarios with zero-overhead computation from already-logged probabilities.

\section{Reproducibility}

Run the complete pipeline with:

\noindent\texttt{DATASET\_FUNC="my\_dataset\_module:iter\_eval"\\
CLASSIFIER\_SPEC="ensemble\_ml\_classifier:EnsembleMLClassifier"\\
make traces \&\& make figs \&\& make tables-vt \&\& make pdf}

All source code and data generation scripts are included in the repository.

\section{Conclusion}

We have presented a comprehensive system for explainable ensemble classification through vote trace analysis. Our approach provides detailed insights into ensemble decision-making while maintaining practical performance characteristics. The combination of timeline visualization, Shapley attribution, and quantitative analysis enables both debugging and interpretability applications for RF signal classification systems.

Future work will explore temporal voting patterns across signal sequences and adaptive ensemble weighting based on attribution feedback.

\bibliographystyle{IEEEtran}
\bibliography{refs}

\end{document}